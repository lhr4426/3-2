{"nbformat":4,"nbformat_minor":0,"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[{"file_id":"1zNUL4aPWau5qq0OPWzSYl_wrb8BDFdVp","timestamp":1698296733406}]}},"cells":[{"cell_type":"markdown","source":["# References: https://github.com/youbeebee/deeplearning_from_scratch"],"metadata":{"id":"xd5ZI3FfFku2"}},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-09-28T02:49:42.894964Z","start_time":"2020-09-28T02:49:42.887964Z"},"id":"2WRoeHk2BRLa"},"source":["수강생분의 이름, 학번을 반영해주세요."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-09-28T02:54:20.871646Z","start_time":"2020-09-28T02:54:15.071169Z"},"id":"p6lZMos0BRLb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698300959625,"user_tz":-540,"elapsed":8,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"d80214fb-5691-492f-f4db-2529b2b7a551"},"source":["id = '20211924'\n","name = '이혜림'\n","print(id, name)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["20211924 이혜림\n"]}]},{"cell_type":"markdown","source":["구글 드라이브 연동"],"metadata":{"id":"MBj1kBpvEjy4"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPGWitzhEjl_","executionInfo":{"status":"ok","timestamp":1698300992432,"user_tz":-540,"elapsed":17562,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"cdb60b16-f945-4b23-83d5-d96a922bf337"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"markdown","source":["폴더 경로 설정"],"metadata":{"id":"Rz9zrCwHE7Y9"}},{"cell_type":"code","source":["workspace_path = '/gdrive/My Drive/Colab Notebooks/AI/AI_week8'  # 과제 파일 업로드한 경로 반영"],"metadata":{"id":"sfZV5eWRE7oG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["폴더 접근 허용"],"metadata":{"id":"mYXRFUrREx63"}},{"cell_type":"code","source":["import sys\n","sys.path.append(workspace_path)"],"metadata":{"id":"NhuEyESNEyL3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["실험결과 재현 함수"],"metadata":{"id":"5zllflSzHh_z"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","\n","def seed_everything(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    # torch.manual_seed(seed)\n","    # if torch.cuda.is_available():\n","    #     torch.cuda.manual_seed(seed)\n","    #     if torch.cuda.device_count() > 1:\n","    #         torch.cuda.manual_seed_all(seed) # if use multi-GPU\n","    #     torch.backends.cudnn.deterministic = True\n","    #     torch.backends.cudnn.benchmark = False"],"metadata":{"id":"5csm7Qh1HiX0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 신경망 학습"],"metadata":{"id":"ahW_aqPWF3yx"}},{"cell_type":"markdown","metadata":{"id":"A24rzsRaNf4G"},"source":["손실함수: 평균제곱오차, 교차 엔트로피 오차"]},{"cell_type":"code","metadata":{"id":"f35aBUozTot0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698300996452,"user_tz":-540,"elapsed":4,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"51982b85-dee6-414f-f8ec-b8cca902ac70"},"source":["import numpy as np\n","'''\n","손실 함수loss function : 신경망 성능의 '나쁨'을 나타내는 지표.\n","일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용\n","'''\n","\n","\n","# 4.2.1 평균 제곱 오차\n","# E = 1/2 * ∑ _k (yk-tk)²\n","# yk : 신경망의 출력\n","# tk : 정답 레이블\n","# k : 데이터의 차원 수\n","def mean_squared_error(y, t):\n","    return 0.5 * np.sum((y-t)**2)\n","\n","\n","# 정답은 '2'\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","\n","# ex1 '2'일 확률이 가장 높다고 추정함(0.6)\n","y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","mse = mean_squared_error(np.array(y), np.array(t))\n","print(mse)  # 0.0975\n","# ex2 '7'일 확률이 가장 높다고 추정함(0.6)\n","y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","mse = mean_squared_error(np.array(y), np.array(t))\n","print(mse)  # 0.5975\n","\n","\n","# 4.2.2 교차 엔트로피 오차\n","# E = -∑ _k (tk * log(yk))\n","# log : 자연로그\n","# yk : 신경망의 출력\n","# tk : 정답 레이블(one-hot encoding)\n","# k : 데이터의 차원 수\n","# 실질적으로 정답일때의 추정의 자연로그를 계산하는 식이 됨\n","def cross_entropy_error(y, t):\n","    delta = 1e-7  # 0일때 -무한대가 되지 않기 위해 작은 값을 더함\n","    return -np.sum(t * np.log(y + delta))\n","\n","\n","# 동일한 계산\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","cee = cross_entropy_error(np.array(y), np.array(t))\n","print(cee)  # 0.510825457099\n","y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","cee = cross_entropy_error(np.array(y), np.array(t))\n","print(cee)  # 2.30258409299\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.09750000000000003\n","0.5975\n","0.510825457099338\n","2.302584092994546\n"]}]},{"cell_type":"markdown","source":["미니배치 교차 엔트로피 오차"],"metadata":{"id":"HhzdIlHKi94z"}},{"cell_type":"code","source":["import numpy as np\n","import sys\n","import os\n","sys.path.append(os.pardir)\n","from dataset.mnist import load_mnist\n","\n","# 4.2.3 미니배치 학습\n","# 훈련 데이터 전체에 대한 오차함수\n","# E = -1/N * ∑ _n (∑ _k (tk * log(yk)))\n","# N : 데이터의 개수\n","# 훈련 데이터 전체에 대한 손실 함수를 계산하기에는 시간이 오래걸리기 때문에\n","# 일부를 추려 전체의 근사치로 이용할 수 있다.\n","(x_train, t_train), (x_test, t_test) = \\\n","    load_mnist(normalize=True, one_hot_label=False)\n","\n","print(x_train.shape)  # (60000, 784)\n","print(t_train.shape)  # 원-핫 인코딩 된 정답 레이블 (60000, 10)\n","\n","# 무작위 10개 추출\n","train_size = x_train.shape[0]\n","batch_size = 10\n","batch_mask = np.random.choice(train_size, batch_size)\n","x_batch = x_train[batch_mask]\n","t_batch = t_train[batch_mask]\n","\n","\n","# 4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","\n","    batch_size = y.shape[0]\n","    return -np.sum(t * np.log(y[np.arange(batch_size), t])) / batch_size\n","\n","\n","# 4.2.5 왜 손실 함수를 설정하는가?\n","# 신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다.\n","# 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.\n","# (매개변수의 미소한 변화에는 거의 반응을 보이지 않고 그 값이 분연속적으로 변화)\n"],"metadata":{"id":"auIqI4SRi9nz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698300998102,"user_tz":-540,"elapsed":1653,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"e8c2b335-6349-4d37-b7f3-8dd191e371a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n","(60000,)\n"]}]},{"cell_type":"markdown","source":["수치 미분"],"metadata":{"id":"EmvpQMRoFTLC"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pylab as plt\n","\n","\n","# 4.3.1 미분\n","# 나쁜 구현 예\n","def numerical_diff_bad(f, x):\n","    h = 10e-50\n","    return (f(x + h) - f(x)) / h\n","# h값이 너무 작아 반올림 오차를 일으킬 수 있음 10e-4정도가 적당하다고 알려짐\n","# 전방 차분에서는 차분이 0이 될 수 없어 오차가 발생\n","#  -> 오차를 줄이기 위해 중심 차분을 사용\n","\n","\n","def numerical_diff(f, x):\n","    h = 10e-4\n","    return (f(x + h) - f(x - h)) / (2 * h)\n","\n","\n","# 4.3.2 수치 미분의 예\n","# y = 0.01x² + 0.1x\n","def function_1(x):\n","    return 0.01*x**2 + 0.1*x\n","\n","\n","x = np.arange(0.0, 20.0, 0.1)  # 0에서 20까지 간격 0.1인 배열 x를 만든다.\n","y = function_1(x)\n","plt.xlabel(\"x\")\n","plt.ylabel(\"f(x)\")\n","plt.plot(x, y)\n","# plt.show()\n","\n","# x = 5, 10일때 미분\n","print(numerical_diff(function_1, 5))   # 0.200000000000089\n","print(numerical_diff(function_1, 10))  # 0.29999999999996696\n","\n","\n","# 접선의 함수를 구하는 함수\n","def tangent_line(f, x):\n","        d = numerical_diff(f, x)\n","        # print(d)\n","        y = f(x) - d*x\n","        return lambda t: d*t + y\n","\n","\n","tf = tangent_line(function_1, 5)\n","y2 = tf(x)\n","plt.plot(x, y2)\n","plt.show()\n","\n","\n","# 4.3.3 편미분\n","# f(x0, x1) = x0² + x1²\n","def function_2(x):\n","    return x[0]**2 + x[1]**2\n","    # or return np.sum(x**2)\n","\n","\n","# x0 = 3, x1 = 4일 때, x0에 대한 편미분을 구하라.\n","def function_tmp1(x0):\n","    return x0**2 + 4.0**2.0\n","\n","\n","# x0 = 3, x1 = 4일 때, x1에 대한 편미분을 구하라.\n","def function_tmp2(x1):\n","    return 3.0**2.0 + x1 * x1\n","\n","\n","print(numerical_diff(function_tmp1, 3.0))  # 5.999999999998451\n","print(numerical_diff(function_tmp2, 4.0))  # 8.000000000000895\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"x1nD-RqBFTcb","executionInfo":{"status":"ok","timestamp":1698300998102,"user_tz":-540,"elapsed":10,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"4b7eaa08-2255-4279-d37f-f55362806e82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.200000000000089\n","0.29999999999996696\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["5.999999999998451\n","8.000000000000895\n"]}]},{"cell_type":"markdown","source":["기울기 계산"],"metadata":{"id":"asO7sXybFXEz"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pylab as plt\n","\n","\n","# 앞 절에서 x0, x1에 대한 편미분을 변수별로 따로 계산했음.\n","# x0, x1의 편미분을 동시에 계산하고 싶다면?\n","# x0 = 3, x1 = 4일 때 (x0, x1) 양쪽의 편미분을 묶어 벡터로 정리한 것을 기울기gradient라고 한다.\n","def numerical_gradient(f, x):\n","    h = 1e-4\n","    grad = np.zeros_like(x)  # x와 형상이 같은 배열을 생성\n","\n","    for idx in range(x.size):\n","        tmp_val = x[idx]\n","        # f(x+h) 계산\n","        x[idx] = tmp_val + h\n","        fxh1 = f(x)\n","\n","        # f(x-h) 계산\n","        x[idx] = tmp_val - h\n","        fxh2 = f(x)\n","\n","        grad[idx] = (fxh1 - fxh2) / (2 * h)\n","        x[idx] = tmp_val  # 값 복원\n","\n","    return grad\n","\n","\n","# f(x0, x1) = x0² + x1²\n","def function_2(x):\n","    return x[0]**2 + x[1]**2\n","    # or return np.sum(x**2)\n","\n","\n","print(numerical_gradient(function_2, np.array([3.0, 4.0])))  # [ 6.  8.]\n","print(numerical_gradient(function_2, np.array([0.0, 2.0])))  # [ 0.  4.]\n","print(numerical_gradient(function_2, np.array([3.0, 0.0])))  # [ 6.  0.]\n","\n","# 4.4.1 경사법(경사 하강법)\n","# x0 = x0 - η*∂f/∂x0\n","# x1 = x1 - η*∂f/∂x1\n","# η(eta) : 갱신하는 양, 학습률learning rate\n","# 위 식을 반복\n","\n","\n","# f:최적화하려는 함수\n","# init_x : 초깃값\n","# lr : 학습률\n","# step_num : 반복횟수\n","def gradient_descent(f, init_x, lr=0.01, step_num=100):\n","    x = init_x\n","    x_history = []\n","\n","    for i in range(step_num):\n","        x_history.append(x.copy())\n","        grad = numerical_gradient(f, x)\n","        x -= lr * grad\n","\n","    return x, np.array(x_history)\n","\n","\n","# 경사법으로 f(x0, x1) = x0² + x1²의 최솟값을 구해라\n","init_x = np.array([-3.0, 4.0])\n","x, x_history = gradient_descent(function_2, init_x, lr=0.1)\n","print(x)  # [ -6.11110793e-10   8.14814391e-10]\n","\n","# 학습률이 너무 큼\n","init_x = np.array([-3.0, 4.0])\n","x, x_history = gradient_descent(function_2, init_x, lr=10.0)\n","print(x)  # [ -2.58983747e+13  -1.29524862e+12] 발산함\n","\n","# 학습률이 너무 작음\n","init_x = np.array([-3.0, 4.0])\n","x, x_history = gradient_descent(function_2, init_x, lr=1e-10)\n","print(x)  # [-2.99999994  3.99999992] 거의 변화 없음\n","\n","# 그래프\n","init_x = np.array([-3.0, 4.0])\n","x, x_history = gradient_descent(function_2, init_x, lr=0.1, step_num=20)\n","\n","plt.plot([-5, 5], [0, 0], '--b')\n","plt.plot([0, 0], [-5, 5], '--b')\n","plt.plot(x_history[:, 0], x_history[:, 1], 'o')\n","\n","plt.xlim(-3.5, 3.5)\n","plt.ylim(-4.5, 4.5)\n","plt.xlabel(\"X0\")\n","plt.ylabel(\"X1\")\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"id":"FnovbeF3FXUT","executionInfo":{"status":"ok","timestamp":1698300998452,"user_tz":-540,"elapsed":355,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"881fc2fc-0900-4cf0-f85f-cf2c499b7de2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[6. 8.]\n","[0. 4.]\n","[6. 0.]\n","[-6.11110793e-10  8.14814391e-10]\n","[-2.58983747e+13 -1.29524862e+12]\n","[-2.99999994  3.99999992]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["학습 알고리즘 구현"],"metadata":{"id":"BJxRcf5JFvUS"}},{"cell_type":"code","source":["'''\n","전제\n","신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.\n","신경망 학습은 다음과 같이 4단계로 수행한다.\n","\n","1단계 - 미니배치\n","훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며,\n","그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n","\n","2단계 - 기울기 산출\n","미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n","기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n","\n","3단계 - 매개변수 갱신\n","가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n","\n","4단계 - 반복\n","1~3단계를 반복한다.\n","\n","데이터를 무작위로 선정하기 때문에 확률적 경사 하강법stochastic gradient descent,\n","SGD라고 부른다.\n","'''\n","import os\n","import numpy as np\n","from common.functions import sigmoid, softmax, cross_entropy_error\n","from common.gradient import numerical_gradient\n","\n","\n","class TwoLayerNet:\n","    \"\"\"\n","    params : 신경망의 매개변수를 보관하는 딕셔너리 변수.\n","    params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향.\n","    params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향.\n","\n","    grad : 기울기를 보관하는 딕셔너리 변수(numerical_gradient()의 반환값)\n","    grads['W1']은 1번째 층의 가중치의 기울기, grads['b1']은 1번째 층의 편향의 기울기.\n","    grads['W2']은 2번째 층의 가중치의 기울기, grads['b2']은 2번째 층의 편향의 기울기.\n","    \"\"\"\n","    # 초기화를 수행한다.\n","    def __init__(self, input_size, hidden_size, output_size,\n","                 weight_init_std=0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * \\\n","            np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * \\\n","            np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","    # 예측(추론)을 수행한다.\n","    def predict(self, x):\n","        W1, W2 = self.params['W1'], self.params['W2']\n","        b1, b2 = self.params['b1'], self.params['b2']\n","\n","        a1 = np.dot(x, W1) + b1\n","        z1 = sigmoid(a1)\n","        a2 = np.dot(z1, W2) + b2\n","        y = softmax(a2)\n","\n","        return y\n","\n","    # 손실 함수의 값을 구한다.\n","    # x : 입력데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","\n","        return cross_entropy_error(y, t)\n","\n","    # 정확도를 구한다.\n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","\n","    # 가중치 매개변수의 기울기를 구한다.\n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","\n","        return grads\n","\n","\n","if __name__ == '__main__':\n","    net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n","    print(net.params['W1'].shape)  # (784, 100)\n","    print(net.params['b1'].shape)  # (100,)\n","    print(net.params['W2'].shape)  # (100, 10)\n","    print(net.params['b2'].shape)  # (10,)\n","\n","    x = np.random.rand(100, 784)  # 더미 입력 데이터(100장 분량)\n","    t = np.random.rand(100, 10)   # 더미 정답 레이블(100장 분량)\n","\n","    grads = net.numerical_gradient(x, t)  # 기울기 계산\n","    # 주의 : 실행하는데 아주 오래걸림\n","    print(grads['W1'].shape)  # (784, 100)\n","    print(grads['b1'].shape)  # (100,)\n","    print(grads['W2'].shape)  # (100, 10)\n","    print(grads['b2'].shape)  # (10,)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7PYfsrL4Fvi6","executionInfo":{"status":"ok","timestamp":1698301219671,"user_tz":-540,"elapsed":221223,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"3deaa159-d5a8-4c07-9313-d07530c78e73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(784, 100)\n","(100,)\n","(100, 10)\n","(10,)\n","(784, 100)\n","(100,)\n","(100, 10)\n","(10,)\n"]}]},{"cell_type":"markdown","source":["# 오차역전파법"],"metadata":{"id":"4-de-2sKF7lJ"}},{"cell_type":"markdown","source":["단순 계층 구현: 곱셈, 덧셈"],"metadata":{"id":"MlKAMjA1F_ME"}},{"cell_type":"code","source":["# 5.4.1 곱셈 계층\n","class MulLayer:\n","    def __init__(self):\n","        self.x = None\n","        self.y = None\n","\n","    def forward(self, x, y):\n","        self.x = x\n","        self.y = y\n","        out = x * y\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * self.y\n","        dy = dout * self.x\n","        return dx, dy\n","\n","\n","# 5.4.2 덧셈 계층\n","class AddLayer:\n","    def __init__(self):\n","        pass\n","\n","    def forward(self, x, y):\n","        out = x + y\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * 1\n","        dy = dout * 1\n","        return dx, dy\n","\n","\n","if __name__ == '__main__':\n","    # 문제1의 예시\n","    apple = 100\n","    apple_num = 2\n","    tax = 1.1\n","\n","    # 계층들\n","    mul_apple_layer = MulLayer()\n","    mul_tax_layer = MulLayer()\n","\n","    # 순전파\n","    apple_price = mul_apple_layer.forward(apple, apple_num)\n","    price = mul_tax_layer.forward(apple_price, tax)\n","\n","    print(price)  # 220.0\n","\n","    # 역전파\n","    dprice = 1\n","    dapple_price, dtax = mul_tax_layer.backward(dprice)\n","    dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","    print(dapple, dapple_num, dtax)  # 2.2 110.0 200\n","\n","    # 문제2의 예시\n","    orange = 150\n","    orange_num = 3\n","\n","    # 계층들\n","    mul_apple_layer = MulLayer()\n","    mul_orange_layer = MulLayer()\n","    add_apple_orange_layer = AddLayer()\n","    mul_tax_layer = MulLayer()\n","\n","    # 순전파\n","    apple_price = mul_apple_layer.forward(apple, apple_num)\n","    orange_price = mul_orange_layer.forward(orange, orange_num)\n","    all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n","    price = mul_tax_layer.forward(all_price, tax)\n","\n","    print(price)  # 715.0\n","\n","    # 역전파\n","    dprice = 1\n","    dall_price, dtax = mul_tax_layer.backward(dprice)\n","    dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n","    dornage, dorange_num = mul_orange_layer.backward(dorange_price)\n","    dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","    print(dapple_num, dapple, dornage, dorange_num, dtax)\n","    # 110.0 2.2 3.3 165.0 650\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m0GQG6u8F-wk","executionInfo":{"status":"ok","timestamp":1698301219671,"user_tz":-540,"elapsed":11,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"58821ad6-828c-4af7-b176-e79733424712"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["220.00000000000003\n","2.2 110.00000000000001 200\n","715.0000000000001\n","110.00000000000001 2.2 3.3000000000000003 165.0 650\n"]}]},{"cell_type":"markdown","source":["활성화 함수, Affine, Softmax 계층 구현\n","- Affine Transformation: W*X + B"],"metadata":{"id":"lJChMYmtGHJP"}},{"cell_type":"code","source":["import numpy as np\n","\n","# 5.5.1 ReLU 계층\n","\"\"\"\n","y = x (x > 0)\n","    0 (x <= 0)\n","∂y/∂x  = 1 (x > 0)\n","         0 (x <= 0)\n","\n","ReLU의 계산 그래프\n","if x > 0\n","x     → relu → y\n","∂L/∂y ← relu ← ∂L/∂y\n","\n","if x <= 0\n","x → relu → y\n","0 ← relu ← ∂L/∂y\n","\"\"\"\n","\n","\n","class Relu:\n","    def __init__(self):\n","        self.mask = None\n","\n","    def forward(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","\n","        return dx\n","\n","\n","# 5.5.2 Sigmoid 계층\n","\"\"\"\n","y = 1 / (1 + exp(-x))\n","\n","시그모이드의 계산 그래프\n","x → × → exp → + → / → y\n","-1↗         1↗\n","\n","1단계\n","'/'노드\n","y = 1/x\n","∂y/∂x = -1/x^2 = -y²\n","상류에서 흘러온 값에 -y^2(순전파의 출력을 제곱하고 마이너스)을 곱해서 하류로 전달 : -∂L/∂y*y²\n","\n","2단계\n","'+'노드\n","상류의 값을 그대로 하류로 전달 : -∂L/∂y*y²\n","\n","3단계\n","'exp'노드\n","y = exp(x)\n","∂y/∂x = exp(x)\n","상류의 값에 순전파 때의 출력(이 경우엔 exp(-x))을 곱해 하류로 전달 : -∂L/∂y*y²*exp(-x)\n","\n","4단계\n","'×'노드\n","순전파 때의 값을 서로 바꿔 곱함(여기서는 * -1) : ∂L/∂y*y²*exp(-x)\n","∂L/∂y*y^2*exp(-x)는 정리하면 ∂L/∂y*y(1-y)가 된다.(순전파의 출력만으로 계산할 수 있다)\n","\n","정리\n","x            → sigmoid → y\n","∂L/∂y*y(1-y) ← sigmoid ← ∂L/∂y\n","\"\"\"\n","\n","\n","class Sigmoid:\n","    def __init__(self):\n","        self.out = None\n","\n","    def forward(self, x):\n","        out = 1 / (1 + np.exp(-x))\n","        self.out = out\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out\n","\n","        return dx\n","\n","\n","if __name__ == '__main__':\n","    x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n","    print(x)\n","    \"\"\"\n","    [[ 1.  -0.5]\n","     [-2.   3. ]]\n","    \"\"\"\n","\n","    mask = (x <= 0)\n","    print(mask)\n","    \"\"\"\n","    [[False  True]\n","     [ True False]]\n","    \"\"\"\n","\n","    # 5.6 Affine/Softmax 계층 구현하기\n","    # 5.6.1 Affine 계층\n","    \"\"\"\n","    신경망의 순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬의 내적을 사용했다.(3.3 참고)\n","    \"\"\"\n","    X = np.random.rand(2)     # 입력\n","    W = np.random.rand(2, 3)  # 가중치\n","    B = np.random.rand(3)     # 편향\n","\n","    print(X.shape)  # (2,)\n","    print(W.shape)  # (2, 3)\n","    print(B.shape)  # (3,)\n","\n","    Y = np.dot(X, W) + B\n","    # 신경망의 순전파 때 수행하는 행렬의 내적은 기하학에서는 어파인 변환이라고 한다.\n","\n","\"\"\"\n","Affine 계층의 계산 그래프\n","X, W, B는 행렬(다차원 배열)\n","\n","1. X ↘    X·W\n","       dot → + → Y\n","2. W ↗ 3.B ↗\n","\n","1. ∂L/∂X = ∂L/∂Y·W^T\n","   (2,)    (3,)  (3,2)\n","2. ∂L/∂W = X^T·∂L/∂Y\n","   (2,3)  (2,1)(1,3)\n","3. ∂L/∂B = ∂L/∂Y\n","   (3,)    (3,)\n","W^T : W의 전치행렬(W가 (2,3)이라면 W^T는(3,2)가 된다.)\n","X = (x0, x1, x2, ..., xn)\n","∂L/∂X = (∂L/∂x0, ∂L/∂x1, ∂L/∂x2, ..., ∂L/∂xn)\n","따라서 X와 ∂L/∂X의 형상은 같다.\n","\"\"\"\n","\n","# 5.6.2 배치용 Affine 계층\n","\"\"\"\n","입력 데이터로 X 하나만이 아니라 데이터 N개를 묶어 순전파하는 배치용 계층을 생각\n","\n","배치용 Affine 계층의 계산 그래프\n","X의 형상이 (N,2)가 됨.\n","\n","1. ∂L/∂X = ∂L/∂Y·W^T\n","   (N,2)   (N,3) (3,2)\n","2. ∂L/∂W = X^T·∂L/∂Y\n","   (2,3)  (2,N)(N,3)\n","3. ∂L/∂B = ∂L/∂Y의 첫 번째 축(0축, 열방향)의 합.\n","   (3,)    (N,3)\n","\n","편향을 더할 때에 주의해야 한다. 순전파 때의 편향 덧셈은 X·W에 대한 편향이\n","각 데이터에 더해진다. 예를 들어 N=2일 경우 편향은 두 데이터 각각에 더해진다.\n","\"\"\"\n","if __name__ == '__main__':\n","    X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n","    B = np.array([1, 2, 3])\n","    print(X_dot_W)\n","    \"\"\"\n","    [[ 0  0  0]\n","     [10 10 10]]\n","    \"\"\"\n","    print(X_dot_W + B)\n","    \"\"\"\n","    [[ 1  2  3]\n","     [11 12 13]]\n","    \"\"\"\n","    \"\"\"\n","    순전파의 편향 덧셈은 각각의 데이터에 더해지므로\n","    역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다.\n","    \"\"\"\n","    dY = np.array([[1, 2, 3], [4, 5, 6]])\n","    print(dY)\n","    \"\"\"\n","    [[1 2 3]\n","     [4 5 6]]\n","    \"\"\"\n","    dB = np.sum(dY, axis=0)\n","    print(dB)  # [5 7 9]\n","    \"\"\"\n","    데이터가 두 개일 때 편향의 역전파는 두 데이터에 대한 미분을 데이터마다\n","    더해서 구한다.\n","    np.sum()에서 0번째 축(데이터를 단위로 한 축. axis=0)에 대해서 합을 구한다.\n","    \"\"\"\n","\n","\n","class Affine:\n","    def __init__(self, W, b):\n","        self.W = W\n","        self.b = b\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        self.x = x\n","        out = np.dot(x, self.W) + self.b\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis=0)\n","\n","        return dx\n","\n","\n","# 5.6.3 Softmax-with-Loss 계층\n","\"\"\"\n","소프트맥스 계층 : 입력 값을 정규화(출력의 합이 1이 되도록 변경)하여 출력\n","학습과 추론 중 학습에서 주로 사용\n","소프트맥스 계층과 손실 함수(교차 엔트로피 오차)를 포함해 계산 그래프를 그림\n","자세한 역전파 계산은 부록A 참고.\n","\n","간소화한 Softmax-with-Loss계층의 계산 그래프\n","a1   →    |         | → y1 → |         |\n","y1 - t1 ← |         |   t1 ↗  | Cross   |\n","a2   →    | Softmax | → y2 → | Entropy | → L\n","y2 - t2 ← |         |   t2 ↗  | Error   | ← 1\n","a3   →    |         | → y3 → |         |\n","y3 - t3 ←               t3 ↗\n","입력 : (a1, a2, a3)\n","정규화된 출력 : (y1, y2, y3)\n","정답 레이블 (t1, t2, t3)\n","손실 : L\n","\n","역전파로 Softmax 계층의 출력과 정답 레이블의 차분 값\n","(y1 - t1, y2 - t2, y2 - t2)이 전달됨.\n","이는 교차 엔트로피 오차 함수가 그렇게 설계되었기 때문.\n","항등 함수의 손실 함수로는 평균 제곱 오차를 사용하는데,\n","그럴 경우 역전파의 결과가 (y1 - t1, y2 - t2, y2 - t2)로 말끔히 떨어짐.\n","\n","ex) 정답 레이블 t = (0, 1, 0) 일 때,\n","소프트맥스가 (0.3, 0.2, 0.5)를 출력했다고 할 때, 소프트맥스 계층의 역전파는\n","(0.3, -0.8, 0.5)로 앞 계층에 큰 오차를 전파하게 됨\n","소프트맥스가 (0.01, 0.99, 0.)을 출력했다면 역전파는 (0.01, -0.01, 0)\n","으로 오차가 작아짐\n","\"\"\"\n","\n","\n","# yk = exp(ak) / ∑(i=1 to n)(exp(ai))\n","def softmax(a):\n","    c = np.max(a)\n","    exp_a = np.exp(a - c)  # 오버플로 대책\n","    sum_exp_a = np.sum(exp_a)\n","    y = exp_a / sum_exp_a\n","\n","    return y\n","\n","\n","def cross_entropy_error(y, t):\n","    delta = 1e-7  # 0일때 -무한대가 되지 않기 위해 작은 값을 더함\n","    return -np.sum(t * np.log(y + delta))\n","\n","\n","class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.loss = None  # 손실\n","        self.y = None     # softmax의 출력\n","        self.t = None     # 정답 레이블(원-핫 벡터)\n","\n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)  # 3.5.2, 4.2.2에서 구현\n","        self.loss = cross_entropy_error(self.y, self.t)\n","\n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","        dx = self.y - self.t / batch_size\n","\n","        return dx\n","\n","\n","if __name__ == '__main__':\n","    swl = SoftmaxWithLoss()\n","    a = np.array([1, 8, 3])   # 비슷하게 맞춤\n","    t = np.array([0, 1, 0])\n","    print(swl.forward(a, t))  # 0.0076206166295\n","    print(swl.backward())     # [ 0.00090496  0.65907491  0.00668679]\n","\n","    a = np.array([1, 3, 8])   # 오차가 큼\n","    print(swl.forward(a, t))  # 5.00760576266\n","    print(swl.backward())   # [  9.04959183e-04 -3.26646539e-01 9.92408247e-01]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_mdv96thGHZY","executionInfo":{"status":"ok","timestamp":1698301219671,"user_tz":-540,"elapsed":7,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"480b7ecc-4896-4e89-fb94-0b449f63ab34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1.  -0.5]\n"," [-2.   3. ]]\n","[[False  True]\n"," [ True False]]\n","(2,)\n","(2, 3)\n","(3,)\n","[[ 0  0  0]\n"," [10 10 10]]\n","[[ 1  2  3]\n"," [11 12 13]]\n","[[1 2 3]\n"," [4 5 6]]\n","[5 7 9]\n","0.007620616629495912\n","[0.00090496 0.65907491 0.00668679]\n","5.0076057626568575\n","[ 9.04959183e-04 -3.26646539e-01  9.92408247e-01]\n"]}]},{"cell_type":"markdown","source":["오차역전파법 구현"],"metadata":{"id":"ch5cu_brGP8M"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from collections import OrderedDict\n","from common.layers import *\n","from common.gradient import numerical_gradient\n","from dataset.mnist import load_mnist\n","\n","# 5.7.1 신경망 학습의 전체 그림\n","\"\"\"\n","(4.5와 동일)\n","전제\n","신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.\n","신경망 학습은 다음과 같이 4단계로 수행한다.\n","\n","1단계 - 미니배치\n","훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며,\n","그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n","\n","2단계 - 기울기 산출\n","미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n","기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n","\n","3단계 - 매개변수 갱신\n","가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n","\n","4단계 - 반복\n","1~3단계를 반복한다.\n","\n","수치 미분과 오차역전파법은 2단계에서 사용\n","수치 미분은 구현은 쉽지만 계산이 오래걸림\n","오차역전파법을 통해 기울기를 효율적이고 빠르게 구할 수 있음\n","\"\"\"\n","\n","# 5.7.2 오차역전파법을 이용한 신경망 구현하기\n","\"\"\"\n","TwoLayerNet 클래스로 구현\n"," * 클래스의 인스턴스 변수\n","params : 신경망의 매개변수를 보관하는 딕셔너리 변수.\n","        params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향.\n","        params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향.\n","layers : 신경망의 계층을 보관하는 순서가 있는 딕셔너리 변수\n","        layers['Affine1'], layers['Relu1'], layers['Affine2']와 같이\n","        각 계층을 순서대로 유지\n","lastLayer : 신경망의 마지막 계층(여기서는 SoftmaxWithLoss)\n","\n"," * 클래스의 메서드\n","__init__(...) : 초기화 수행\n","predict(x) : 예측(추론)을 수행한다. x는 이미지 데이터\n","loss(x, t) : 손실함수의 값을 구한다. x는 이미지 데이터, t는 정답 레이블\n","accuracy(x, t) : 정확도를 구한다.\n","numerical_gradient(x, t) : 가중치 매개변수의 기울기를 수치 미분으로 구함(앞 장과 같음)\n","gradient(x, t) : 가중치 매개변수의 기울기를 오차역전파법으로 구함\n","\"\"\"\n","\n","\n","class TwoLayerNet:\n","    def __init__(self, input_size, hidden_size, output_size,\n","        weight_init_std=0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","        # 계층 생성\n","        self.layers = OrderedDict()\n","        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n","        self.layers['Relu1'] = Relu()\n","        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n","        self.lastLayer = SoftmaxWithLoss()\n","\n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","\n","        return x\n","\n","    # x : 입력 데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","        return self.lastLayer.forward(y, t)\n","\n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if t.ndim != 1:\n","            t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","\n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","\n","        return grads\n","\n","    def gradient(self, x, t):\n","        # 순전파\n","        self.loss(x, t)\n","\n","        # 역전파\n","        dout = 1\n","        dout = self.lastLayer.backward(dout)\n","\n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 결과 저장\n","        grads = {}\n","        grads['W1'] = self.layers['Affine1'].dW\n","        grads['b1'] = self.layers['Affine1'].db\n","        grads['W2'] = self.layers['Affine2'].dW\n","        grads['b2'] = self.layers['Affine2'].db\n","\n","        return grads\n","\n","\n","\"\"\"\n","신경망의 계층을 순서가 있는 딕셔너리에서 보관,\n","따라서 순전파때는 추가한 순서대로 각 계층의 forward()를 호출하기만 하면 된다.\n","역전파때는 계층을 반대 순서로 호출하기만 하면 된다.\n","신경망의 구성 요소를 모듈화하여 계층으로 구현했기 때문에 구축이 쉬워진다.\n","\"\"\"\n","\n","\n","# 5.7.3 오차역전파법으로 구한 기울기 검증하기\n","\"\"\"\n","기울기를 구하는데는 두 가지 방법이 있다.\n","1. 수치 미분 : 느리다. 구현이 쉽다.\n","2. 해석적으로 수식을 풀기(오차 역전파법) : 빠르지만 실수가 있을 수 있다.\n","두 기울기 결과를 비교해서 오차역전파법을 제대로 구현했는지 검증한다.\n","이 작업을 기울기 확인gradient check라고 한다.\n","\"\"\"\n","if __name__ == '__main__':\n","    # 데이터 읽기\n","    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","    network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","    x_batch = x_train[:3]\n","    t_batch = t_train[:3]\n","\n","    grad_numerical = network.numerical_gradient(x_batch, t_batch)\n","    grad_backprop = network.gradient(x_batch, t_batch)\n","\n","    # 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n","    for key in grad_numerical.keys():\n","        diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n","        print(key + \":\" + str(diff))\n","        \"\"\"\n","        수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작다.\n","        실수 없이 구현되었을 확률이 높다.\n","        정밀도가 유한하기 때문에 오차가 0이 되지는 않는다.\n","        \"\"\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QU1OaSjkGPwV","executionInfo":{"status":"ok","timestamp":1698301232107,"user_tz":-540,"elapsed":12440,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"c78cdced-709d-463f-ef73-0d4f225904c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["W1:2.469785770740274e-13\n","b1:1.0939865549910999e-12\n","W2:9.40232936104922e-13\n","b2:1.201261340399995e-10\n"]}]},{"cell_type":"markdown","source":["오차역전파법을 사용한 학습 구현"],"metadata":{"id":"LUez16mqGXdp"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from dataset.mnist import load_mnist\n","\n","# 하이퍼 파라미터\n","iters_num = 10000  # 반복횟수\n","train_size = x_train.shape[0]\n","batch_size = 100  # 미니배치 크기\n","learning_rate = 0.1\n","seed = 0  # 실험결과 재현을 위함, 수정 불가\n","seed_everything(seed)\n","\n","# 데이터 읽기\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","# 1에폭당 반복 수\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","best_test_acc = 0\n","for i in range(iters_num):\n","    # print(i)\n","    # 미니배치 획득\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 오차역전파법으로 기울기 계산\n","    grad = network.gradient(x_batch, t_batch)\n","\n","    # 매개변수 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]\n","\n","    # 학습 경과 기록\n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","\n","    # 1에폭 당 정확도 계산\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n","        if test_acc > best_test_acc:\n","            best_test_acc = max(test_acc, best_test_acc)\n","\n","print(\"## best test acc | \" + str(best_test_acc))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R5XLKFKAGXuh","executionInfo":{"status":"ok","timestamp":1698301314267,"user_tz":-540,"elapsed":82163,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"c7f0f0e6-4923-4f29-86e3-4a5d0ad53bd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train acc, test acc | 0.109, 0.1136\n","train acc, test acc | 0.9051333333333333, 0.9069\n","train acc, test acc | 0.92165, 0.9212\n","train acc, test acc | 0.9334166666666667, 0.9336\n","train acc, test acc | 0.9450166666666666, 0.943\n","train acc, test acc | 0.9513333333333334, 0.9482\n","train acc, test acc | 0.9566166666666667, 0.9532\n","train acc, test acc | 0.9614166666666667, 0.9572\n","train acc, test acc | 0.9634166666666667, 0.9582\n","train acc, test acc | 0.9661166666666666, 0.9613\n","train acc, test acc | 0.9697333333333333, 0.9627\n","train acc, test acc | 0.9711, 0.9635\n","train acc, test acc | 0.9733, 0.9669\n","train acc, test acc | 0.9743833333333334, 0.9662\n","train acc, test acc | 0.9755, 0.9684\n","train acc, test acc | 0.97745, 0.9689\n","train acc, test acc | 0.9785833333333334, 0.9699\n","## best test acc | 0.9699\n"]}]},{"cell_type":"markdown","metadata":{"id":"nRP04ogeUJkm"},"source":["# 실습 과제: 개선시도 2가지, 0.9699 보다 높은 test accuracy 달성\n","아래의 2가지를 모두 개선 시도하여 0.9699 보다 높은 test accuracy 달성하시오. 단, PyTorch, 학습된 모델, CNN 사용 불가합니다.\n","\n","* 1) 네트워크 구조 개선(아래의 TwoLayerNet 계층 수정): Affine 최소 3개 이상, ReLU 최소 2개 이상 사용\n"," * Affine 개수에 맞게 가중치 초기화, gradient 계산, 매개변수 갱신 부분도 수정할 것\n","  * 개선내용 설명: (본인의 개선내용 작성하기)\n","* 2) 학습 하이퍼파라미터 변경\n"," * 개선내용 설명: (본인의 개선내용 작성하기)"]},{"cell_type":"markdown","source":["오차역전파법 구현"],"metadata":{"id":"qDOdPYaCIba6"}},{"cell_type":"markdown","source":["네트워크 구조 개선 : Affine을 3개, Relu를 2개 사용함.  \n","784(input) -> 50 -> 40(추가) -> 10 (output)  \n","학습 파라미터 변경 : 반복 횟수는 늘리고, 배치 사이즈와 학습률은 낮췄음.  \n","최고 성능 0.9705 달성"],"metadata":{"id":"FU_z_Vbsp-eP"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from collections import OrderedDict\n","from common.layers import *\n","from common.gradient import numerical_gradient\n","from dataset.mnist import load_mnist\n","\n","# 5.7.1 신경망 학습의 전체 그림\n","\"\"\"\n","(4.5와 동일)\n","전제\n","신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.\n","신경망 학습은 다음과 같이 4단계로 수행한다.\n","\n","1단계 - 미니배치\n","훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며,\n","그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n","\n","2단계 - 기울기 산출\n","미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n","기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n","\n","3단계 - 매개변수 갱신\n","가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n","\n","4단계 - 반복\n","1~3단계를 반복한다.\n","\n","수치 미분과 오차역전파법은 2단계에서 사용\n","수치 미분은 구현은 쉽지만 계산이 오래걸림\n","오차역전파법을 통해 기울기를 효율적이고 빠르게 구할 수 있음\n","\"\"\"\n","\n","# 5.7.2 오차역전파법을 이용한 신경망 구현하기\n","\"\"\"\n","TwoLayerNet 클래스로 구현\n"," * 클래스의 인스턴스 변수\n","params : 신경망의 매개변수를 보관하는 딕셔너리 변수.\n","        params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향.\n","        params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향.\n","layers : 신경망의 계층을 보관하는 순서가 있는 딕셔너리 변수\n","        layers['Affine1'], layers['Relu1'], layers['Affine2']와 같이\n","        각 계층을 순서대로 유지\n","lastLayer : 신경망의 마지막 계층(여기서는 SoftmaxWithLoss)\n","\n"," * 클래스의 메서드\n","__init__(...) : 초기화 수행\n","predict(x) : 예측(추론)을 수행한다. x는 이미지 데이터\n","loss(x, t) : 손실함수의 값을 구한다. x는 이미지 데이터, t는 정답 레이블\n","accuracy(x, t) : 정확도를 구한다.\n","numerical_gradient(x, t) : 가중치 매개변수의 기울기를 수치 미분으로 구함(앞 장과 같음)\n","gradient(x, t) : 가중치 매개변수의 기울기를 오차역전파법으로 구함\n","\"\"\"\n","\n","\n","class TwoLayerNet:\n","    def __init__(self, input_size, hidden_size, output_size,\n","        weight_init_std=0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, hidden_size - 10)\n","        self.params['b2'] = np.zeros(hidden_size - 10)\n","\n","        self.params['W3'] = weight_init_std * np.random.randn(hidden_size - 10, output_size)\n","        self.params['b3'] = np.zeros(output_size)\n","\n","\n","        # 계층 생성\n","        self.layers = OrderedDict()\n","        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n","        self.layers['Relu1'] = Relu()\n","        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n","        self.layers['Relu2'] = Relu()\n","        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n","\n","        self.lastLayer = SoftmaxWithLoss()\n","\n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","        return x\n","\n","    # x : 입력 데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","        return self.lastLayer.forward(y, t)\n","\n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if t.ndim != 1:\n","            t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","\n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n","        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n","\n","        return grads\n","\n","    def gradient(self, x, t):\n","        # 순전파\n","        self.loss(x, t)\n","\n","        # 역전파\n","        dout = 1\n","        dout = self.lastLayer.backward(dout)\n","\n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 결과 저장\n","        grads = {}\n","        grads['W1'] = self.layers['Affine1'].dW\n","        grads['b1'] = self.layers['Affine1'].db\n","        grads['W2'] = self.layers['Affine2'].dW\n","        grads['b2'] = self.layers['Affine2'].db\n","        grads['W3'] = self.layers['Affine3'].dW\n","        grads['b3'] = self.layers['Affine3'].db\n","\n","        return grads\n","\n","\n","\"\"\"\n","신경망의 계층을 순서가 있는 딕셔너리에서 보관,\n","따라서 순전파때는 추가한 순서대로 각 계층의 forward()를 호출하기만 하면 된다.\n","역전파때는 계층을 반대 순서로 호출하기만 하면 된다.\n","신경망의 구성 요소를 모듈화하여 계층으로 구현했기 때문에 구축이 쉬워진다.\n","\"\"\"\n","\n","\n","# 5.7.3 오차역전파법으로 구한 기울기 검증하기\n","\"\"\"\n","기울기를 구하는데는 두 가지 방법이 있다.\n","1. 수치 미분 : 느리다. 구현이 쉽다.\n","2. 해석적으로 수식을 풀기(오차 역전파법) : 빠르지만 실수가 있을 수 있다.\n","두 기울기 결과를 비교해서 오차역전파법을 제대로 구현했는지 검증한다.\n","이 작업을 기울기 확인gradient check라고 한다.\n","\"\"\"\n","if __name__ == '__main__':\n","    # 데이터 읽기\n","    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","    network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","    x_batch = x_train[:3]\n","    t_batch = t_train[:3]\n","\n","    grad_numerical = network.numerical_gradient(x_batch, t_batch)\n","    grad_backprop = network.gradient(x_batch, t_batch)\n","\n","    # 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n","    for key in grad_numerical.keys():\n","        diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n","        print(key + \":\" + str(diff))\n","        \"\"\"\n","        수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작다.\n","        실수 없이 구현되었을 확률이 높다.\n","        정밀도가 유한하기 때문에 오차가 0이 되지는 않는다.\n","        \"\"\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698304876718,"user_tz":-540,"elapsed":13867,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"7c3c6ddf-e746-4b94-d8f6-89e4d0b1732e","id":"gqMjIFdHIba6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["W1:2.166338784209356e-13\n","b1:9.264464977104927e-13\n","W2:4.944369476663982e-13\n","b2:8.269401249915223e-13\n","W3:7.772465688790799e-13\n","b3:1.199040935984108e-10\n"]}]},{"cell_type":"markdown","source":["오차역전파법을 사용한 학습 구현"],"metadata":{"id":"p-29NkLzIba7"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from dataset.mnist import load_mnist\n","\n","# 하이퍼 파라미터\n","iters_num = 20000  # 반복횟수 (10000 -> 20000 수정)\n","train_size = x_train.shape[0]\n","batch_size = 50  # 미니배치 크기 (100 -> 50 수정)\n","learning_rate = 0.05 # (0.1 -> 0.05 수정)\n","seed = 0  # 실험결과 재현을 위함, 수정 불가\n","seed_everything(seed)\n","\n","# 데이터 읽기\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","# 1에폭당 반복 수\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","best_test_acc = 0\n","for i in range(iters_num):\n","    # print(i)\n","    # 미니배치 획득\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 오차역전파법으로 기울기 계산\n","    grad = network.gradient(x_batch, t_batch)\n","\n","    # 매개변수 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n","        network.params[key] -= learning_rate * grad[key]\n","\n","    # 학습 경과 기록\n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","\n","    # 1에폭 당 정확도 계산\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n","        if test_acc > best_test_acc:\n","            best_test_acc = max(test_acc, best_test_acc)\n","\n","print(\"## best test acc | \" + str(best_test_acc))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698304950289,"user_tz":-540,"elapsed":68915,"user":{"displayName":"이혜림컴퓨터공학과","userId":"12232423623052772806"}},"outputId":"4c33456c-7183-440d-d4ff-dc721d944c54","id":"n_wSz0YuIba7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train acc, test acc | 0.10441666666666667, 0.1028\n","train acc, test acc | 0.47151666666666664, 0.4769\n","train acc, test acc | 0.87875, 0.8814\n","train acc, test acc | 0.91915, 0.9206\n","train acc, test acc | 0.93935, 0.9366\n","train acc, test acc | 0.952, 0.9465\n","train acc, test acc | 0.9541833333333334, 0.9475\n","train acc, test acc | 0.9648333333333333, 0.9598\n","train acc, test acc | 0.9669, 0.9608\n","train acc, test acc | 0.9705666666666667, 0.9615\n","train acc, test acc | 0.9736, 0.965\n","train acc, test acc | 0.9754, 0.9666\n","train acc, test acc | 0.9776666666666667, 0.9666\n","train acc, test acc | 0.9784333333333334, 0.9643\n","train acc, test acc | 0.9807666666666667, 0.9645\n","train acc, test acc | 0.9832833333333333, 0.9705\n","train acc, test acc | 0.9852, 0.9702\n","## best test acc | 0.9705\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Sw8EUyMqQqKn"},"execution_count":null,"outputs":[]}]}